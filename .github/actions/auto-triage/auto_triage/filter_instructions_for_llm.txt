Filter Stage Playbook
=====================

Purpose
-------
Identify the newest deterministic failure signature and gather the associated commit window. This stage must be exhaustive when collecting evidence, but its outputs are limited to the source metadata the next model will consume. **You must never analyze or triage commits.** As soon as you either (a) confirm a deterministic error and download the commits between the last passing and first failing run or (b) conclude no deterministic error exists, your work is finished. Leave all reasoning about which commits matter to the next model.

Available Workspace
-------------------
- You are operating inside `.auto_triage/`.
- Run helper scripts from this directory (e.g., `./get_logs.sh`, `./download_data_between_commits.sh`).
- Source files live under `./workspace/`.
- Data artifacts should be written to `./auto_triage/data/` (also symlinked at `./data/`).

Workflow Overview
-----------------
1. Inspect `./data/subjob_runs.json` to understand the run history:
   - The JSON array structure: `[oldest success, oldest failure, ..., newest failure]`
   - Each run object has a `run_number` field (0, 1, 2, ...) that equals its array index.
   - Each run has a `status` field: `"success"` or `"failure"`.
   - **Important:** `run_number` increases as failures get newer (higher number = newer failure).
   - The array is built by `find_boundaries.sh` which:
     - Sorts successes by `completed_at` (ascending) and takes the oldest → gets `run_number: 0`
     - Sorts failures by `completed_at` (ascending, oldest first) → get `run_number: 1, 2, 3, ...`
     - Final order: `[oldest success (run_number 0), oldest failure (run_number 1), ..., newest failure (highest run_number)]`
   - You can verify this by checking the `completed_at` timestamp field: higher `run_number` = later timestamp.
   
   Example structure (based on actual data):
   ```json
   [
     { "run_number": 0, "status": "success", "commit": "abc123", "completed_at": "2025-12-03T02:00:00Z", ... },  // Most recent (newest success)
     { "run_number": 7, "status": "failure", "commit": "ad8d0d3...", "completed_at": "2025-12-03T01:33:50Z", ... },  // Newest failure
     { "run_number": 6, "status": "failure", "commit": "5e09214...", "completed_at": "2025-12-03T01:16:13Z", ... },
     { "run_number": 1, "status": "failure", "commit": "514ae86...", "completed_at": "2025-12-02T23:15:04Z", ... }   // Oldest failure (lowest run_number)
   ]
   ```
   Note: In this example, run_number 7 is newer than run_number 1. Higher run_number = newer run.

2. Identify the runs you need:
   - **Last successful run:** Find the run object with `"status": "success"`. This is your baseline. It will have the highest `run_number` among successful runs.
   - **Oldest failing run:** Among all runs with `"status": "failure"`, find the one with the **lowest `run_number`** (furthest from index 0, earliest timestamp). This is the first failure chronologically.
   - **Two most recent failing runs:** Among all runs with `"status": "failure"`, find the two with the **highest `run_number`** values (closest to index 0, latest timestamps). If there's only one failure, download just that one.

3. Download evidence exactly as follows:
   - Download the **oldest failing run** (lowest `run_number` among failures).
   - Download the **two most recent failing runs** (highest `run_number` values among failures).
   - If fewer than three failures exist total, download every failing run.
   - For each run, call `./get_annotations.sh <job_url>` first. If annotations lack actionable detail (common), immediately fall back to `./get_logs.sh <job_url>`.
3. Ignore infrastructure/setup failures (e.g., runner disconnects, missing dependencies, GitHub outages). Such runs must not influence determinism decisions.
4. Determine whether there is a repeatable deterministic failure signature:
   - Prefer an error that appears in multiple runs with the same stack trace or message.
   - If only one test-executing run exists, treat that signature as deterministic if it clearly points to tt-metal code.
   - **Important:** It is perfectly acceptable and expected for failures to be non-deterministic. If failures appear to be random, infrastructure-related, or unrelated to tt-metal code changes, this is a valid finding. Do not force determinism where none exists.
5. Save the deterministic error snippet (2–6 lines) into `./auto_triage/data/error_message.txt`. If no deterministic failure can be found, write an empty file.

Commit Metadata Collection
--------------------------
6. **Only proceed with commit collection if you identified a deterministic failure that appears related to tt-metal code changes.** If the failures are non-deterministic, infrastructure-related, or clearly unrelated to tt-metal (e.g., flaky tests, runner issues, external dependencies), skip this step entirely and leave `commit_info.json` as an empty array (`[]`). Even when you do download commits, you are gathering metadata only—do **not** inspect diffs, reason about relevance, or guess which commits might be responsible.
7. Locate the commit hashes from the runs you identified:
   - Use the `commit` field from the **last successful run** (the one with `"status": "success"`).
   - Use the `commit` field from the **oldest failing run** (the failure with the lowest `run_number`).
   - Invoke:
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> ./auto_triage/data/commit_info.json
   ```
   - If the commit window has ≤10 commits, the script will download them directly and write to `commit_info.json`.
   - If the commit window has >10 commits, the script will output `BATCH_COUNT=<number>` (e.g., `BATCH_COUNT=5` for 50 commits).
   - When `BATCH_COUNT` is reported, you must run the batch script for each batch index from `0` to `BATCH_COUNT-1`:
     ```
     ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> ./auto_triage/data/commit_info.json
     ```
     - Example: If `BATCH_COUNT=5`, run:
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 0 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 1 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 2 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 3 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 4 ./auto_triage/data/commit_info.json`
     - **Important:** Use the same output file (`./auto_triage/data/commit_info.json`) for all batch calls; results will be appended.
     - Each batch processes up to 10 commits (batches are zero-indexed).

Special Cases
-------------
- **Purely non-deterministic / infra failures:** If every failing run is due to setup/infrastructure noise, random test flakiness, or issues clearly unrelated to tt-metal code changes, **this is a valid and acceptable outcome.** Skip downloading commits entirely. Write an empty JSON array (`[]`) to `./auto_triage/data/commit_info.json` and leave `error_message.txt` blank. This signals Case 3 to the main model, which is the correct classification for non-deterministic failures.
- **No clear deterministic failure:** If you cannot identify a repeatable failure signature across multiple runs, or if failures appear random/unrelated to code changes, treat this as non-deterministic. Leave `commit_info.json` as `[]` and `error_message.txt` blank.
- **Commit span too large (>100) or metadata unavailable:** If the helper scripts refuse to download (or logs for the oldest failing run are missing), write a JSON string into `commit_info.json`, e.g. `"too many commits to analyze; default to Case 2 unless other evidence suggests Case 3"`. Still populate `error_message.txt` with the clearest deterministic message you found (or leave it empty if none were available).

Outputs
-------
- `./auto_triage/data/commit_info.json`
  - Preferred format is a JSON array of commit objects with metadata (commit hash, PR info, authors, approvers, Copilot overview, etc.).
  - **An empty array (`[]`) is a valid and expected output** when failures are non-deterministic or unrelated to tt-metal code changes. Do not force commit collection when evidence suggests the issue is external.
  - The file may also be a JSON string for the "too many commits" scenario described above.
- `./auto_triage/data/error_message.txt`
  - Contains the deterministic failure excerpt (or is empty if determinism could not be established or failures are non-deterministic).

What **Not** to Do
------------------
- Do **not** produce `explanation.md`, `slack_message.json`, or any Case outputs.
- Do **not** spend tokens summarizing commits; simply gather metadata.
- Do **not** interpret commit relevance or hypothesize about root causes—download the metadata and stop.
- Do **not** delete or rename any existing automation scripts.
- Avoid running the LLM twice; this stage is the only LLM invocation before the main analysis.

The main model will perform the final reasoning using the commit list and the captured error message.

