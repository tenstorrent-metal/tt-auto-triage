Filter Stage Playbook
=====================

Purpose
-------
Identify the newest deterministic failure signature and gather the associated commit window. This stage must be exhaustive when collecting evidence, but its outputs are limited to the source metadata the next model will consume. **You must never analyze or triage commits.** As soon as you either (a) confirm a deterministic error and download the commits between the last passing and first failing run or (b) conclude no deterministic error exists, your work is finished. Leave all reasoning about which commits matter to the next model.

Available Workspace
-------------------
- You are operating inside `.auto_triage/`.
- Run helper scripts from this directory (e.g., `./get_logs.sh`, `./download_data_between_commits.sh`).
- Source files live under `./workspace/`.
- Data artifacts should be written to `./auto_triage/data/` (also symlinked at `./data/`).

Cancellation Lever
------------------
- You can terminate the workflow early by writing to `./cancel.json`.
- Default shape:
  ```json
  { "should_cancel": false, "message": "" }
  ```
- Set `should_cancel` to `true` and populate `message` with whatever your justification for canceling is. Here are some possible examples:
  1. `"the failure is clearly non-deterministic/infra noise and would inevitably become Case 3"`
  2. `"this run was invoked in error. the job/workflow inputs are wrong (subjob never existed, boundaries could not even start) and no triage is possible."`
- When cancelling:
  - Write `jq -n --arg msg "<phrase>" '{should_cancel: true, message: $msg}' > ./cancel.json`.
  - You may skip commit downloads entirely; leave `./auto_triage/data/commit_info.json` as `[]` and `error_message.txt` blank unless a deterministic snippet is already captured.
  - Do **not** run any additional helper scripts after setting the cancel flag—exit as soon as artifacts are finalized.
  - If you decide **not** to cancel, leave `cancel.json` with `should_cancel: false` (the default file already satisfies this).

Workflow Overview
-----------------
1. Inspect `./data/subjob_runs.json` to understand the run history:
   - Preferred/new structure:
     ```json
     {
       "status": "ok" | "failure_limit_exceeded" | "no_success_found",
       "message": "<human readable summary>",
       "runs": [
         { "run_number": 0, "status": "success", ... },
         { "run_number": 1, "status": "failure", ... },
         ...
       ]
     }
     ```
   - Legacy structure (older artifacts): the file may instead be a **raw JSON array** of run objects with no `status`/`message`. When you encounter this format, treat the array itself as `runs` and assume `status: "ok"` (no special fallback).
   - `runs` is the array you previously read; it is ordered `[oldest success, oldest failure, ..., newest failure]`.
   - Each run object has:
     - `run_number`: 0, 1, 2, … (higher number = newer failure).
     - `status`: `"success"` or `"failure"`.
     - `run_url`, `job_url`, `commit`, `completed_at`, etc.
   - `status` at the top level (when present) tells you whether the boundary search hit a fallback:
     - `"ok"` → normal behavior.
     - `"failure_limit_exceeded"` → 30 failed runs were scanned without finding a successful run; you **must** follow the fallback in *Special Cases*.
     - `"no_success_found"` → no successes exist in the window; treat this the same as the failure-limit fallback.
     - If the file was a bare array (legacy), default to `"ok"`.
   - Example (truncated):
     ```json
     {
       "status": "ok",
       "message": "",
       "runs": [
         { "run_number": 0, "status": "success", "commit": "abc123", "completed_at": "2025-12-03T02:00:00Z" },
         { "run_number": 7, "status": "failure", "commit": "ad8d0d3...", "completed_at": "2025-12-03T01:33:50Z" },
         { "run_number": 6, "status": "failure", "commit": "5e09214...", "completed_at": "2025-12-03T01:16:13Z" }
       ]
     }
     ```
     Here `run_number` 7 is newer than 6, which is newer than 1, etc.

2. Identify the runs you need:
   - **Last successful run:** In `runs`, find the entry with `"status": "success"`; this is your baseline and will have the highest `run_number` among successes.
   - **Oldest failing run:** Among `"failure"` entries, pick the **lowest `run_number`**; this is the first failure chronologically.
   - **Two most recent failing runs:** Choose the two `"failure"` entries with the **highest `run_number"` values (if fewer exist, use all failures).

3. Download evidence exactly as follows:
   - Download the **oldest failing run** (lowest `run_number` among failures).
   - Download the **two most recent failing runs** (highest `run_number` values among failures).
   - If fewer than three failures exist total, download every failing run.
   - For each run, call `./get_annotations.sh <job_url>` first. If annotations lack actionable detail (common), immediately fall back to `./get_logs.sh <job_url>`.
3. Ignore infrastructure/setup failures (e.g., runner disconnects, missing dependencies, GitHub outages). Such runs must not influence determinism decisions.
4. Determine whether there is a repeatable deterministic failure signature:
   - Prefer an error that appears in multiple runs with the same stack trace or message.
   - If only one test-executing run exists, treat that signature as deterministic if it clearly points to tt-metal code.
   - **Important:** It is perfectly acceptable and expected for failures to be non-deterministic. If failures appear to be random, infrastructure-related, or unrelated to tt-metal code changes, this is a valid finding. Do not force determinism where none exists.
   - **Check Known Patterns:** A file named `non-deterministic-error-examples.json` may exist in your current directory. You **must** use `grep` or `rg` to search this file for key phrases from your candidate error message.
     - Run: `grep -F "substring from error" non-deterministic-error-examples.json` (or use `rg`).
     - The file format is:
       ```json
       {
         "examples": [
           {
             "job": "some example job",
             "test": "tests/ttnn/unit_tests/...",
             "error_message": "RuntimeError: TT_FATAL: ..."
           }
         ]
       }
       ```
     - Remember, the job and test that a past non deterministic issue appeared on is less important then the error itself. Often very different jobs and tests will fail as a result of the same non-deterministic issue
     - Remember, you may have to fuzzy match or imply that two errors are simimlar as every test will produce slightly different output
     - If you find a matching error pattern in that file, treat the failure as non-deterministic/infra noise and either cancel the triage run (see Cancellation Lever), or look at other job failures if they exist.
5. Save the deterministic error snippet (2–6 lines) into `./auto_triage/data/error_message.txt`. If no deterministic failure can be found, write an empty file.

Commit Metadata Collection
--------------------------
6. **Only proceed with commit collection if you identified a deterministic failure that appears related to tt-metal code changes.** If the failures are non-deterministic, infrastructure-related, or clearly unrelated to tt-metal (e.g., flaky tests, runner issues, external dependencies), skip this step entirely and leave `commit_info.json` as an empty array (`[]`). Even when you do download commits, you are gathering metadata only—do **not** inspect diffs, reason about relevance, or guess which commits might be responsible.
7. Locate the commit hashes from the runs you identified:
   - Use the `commit` field from the **last successful run** (the one with `"status": "success"`).
   - Use the `commit` field from the **oldest failing run** (the failure with the lowest `run_number`).
   - Invoke:
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> ./auto_triage/data/commit_info.json
   ```
   - If the commit window has ≤10 commits, the script will download them directly and write to `commit_info.json`.
   - If the commit window has >10 commits, the script will output `BATCH_COUNT=<number>` (e.g., `BATCH_COUNT=5` for 50 commits).
   - When `BATCH_COUNT` is reported, you must run the batch script for each batch index from `0` to `BATCH_COUNT-1`:
     ```
     ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> ./auto_triage/data/commit_info.json
     ```
     - Example: If `BATCH_COUNT=5`, run:
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 0 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 1 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 2 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 3 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 4 ./auto_triage/data/commit_info.json`
     - **Important:** Use the same output file (`./auto_triage/data/commit_info.json`) for all batch calls; results will be appended.
     - Each batch processes up to 10 commits (batches are zero-indexed).
8. **Verify and backfill commit coverage:**
   - Capture the expected commit count via `git rev-list --count <good_commit>..<bad_commit>` (this equals the batch controller’s `Commits in range`).
   - After running all batches, check `jq 'length' ./auto_triage/data/commit_info.json`.
   - If counts match, proceed. If not:
     1. **Retry failing batches** up to two additional times (you may re-run the batch script for the specific index or narrow the commit window to just the problematic span).
     2. **Manually backfill any remaining missing commits** before giving up:
        - List the missing SHAs with `git log --oneline --reverse <good_commit>..<bad_commit>` and compare against the collected `commit_short` values (e.g., using `comm` or `rg`).
        - For each missing SHA:
          - Extract the PR number from the commit message (`git log -1 --format="%s" <sha>`).
          - Use `gh api repos/tenstorrent/tt-metal/pulls/<pr_number>` and `gh api repos/tenstorrent/tt-metal/pulls/<pr_number>/reviews` to gather the same metadata fields the batch script writes (`pr_title`, `pr_url`, `authors`, `approvers`, Copilot overview, etc.). You may reuse helper functions from `download_data_between_commits_batch.sh` by invoking that script with a reduced range that only contains the missing commit; if that still fails, construct the JSON entry manually using `jq -n '{...}'` and append it with `jq '. += [<entry>]' commit_info.json`.
          - Ensure the manually-added objects follow the existing schema (`commit`, `commit_short`, `pr_number`, `authors`, `approvers`, `copilot_overview`, etc.).
     3. Only after retries **and** manual attempts fail for specific commits should you fall back to the partial-data string described below.

Special Cases
-------------
- **Purely non-deterministic / infra failures:** If every failing run is due to setup/infrastructure noise, random test flakiness, or issues clearly unrelated to tt-metal code changes, **this is a valid and acceptable outcome.** Skip downloading commits entirely. Write an empty JSON array (`[]`) to `./auto_triage/data/commit_info.json` and leave `error_message.txt` blank. This signals Case 3 to the main model, which is the correct classification for non-deterministic failures.
- When you are confident the situation above applies and no further analysis is needed, set `cancel.json` with `message: "this is non-deterministic on the output report"` so the pipeline stops after the filter stage.
- **No clear deterministic failure:** If you cannot identify a repeatable failure signature across multiple runs, or if failures appear random/unrelated to code changes, treat this as non-deterministic. Leave `commit_info.json` as `[]` and `error_message.txt` blank.
- **Commit span too large (>100) or metadata unavailable:** If the helper scripts refuse to download (or logs for the oldest failing run are missing), write a JSON string into `commit_info.json`, e.g. `"too many commits to analyze; default to Case 2 unless other evidence suggests Case 3"`. Still populate `error_message.txt` with the clearest deterministic message you found (or leave it empty if none were available).
- **Failure history too long (`subjob_runs.json.status` = `failure_limit_exceeded` or `no_success_found`):** `find_boundaries.sh` already scanned 30+ failed runs without finding a success, so there is no trustworthy commit boundary.
  - Do **not** attempt to download commits; you lack the "good" commit.
  - Still capture the deterministic error message (if any) in `error_message.txt`.
  - Write a JSON string to `commit_info.json`, for example: `"failure_limit_exceeded: more than 30 failed runs without a success; default to Case 2 or Case 3."`
  - This string instructs the main analysis to pick either Case 2 (if the error is deterministic and clearly tt-metal related) or Case 3 (if it appears external/non-deterministic).
- If the workflow/job inputs were incorrect or no subjob could be found at all, set `cancel.json` with `message: "this run was invoked in error"` so downstream stages are skipped entirely.
- **Partial commit downloads despite retries/manual backfill:** When specific commits cannot be retrieved even after the manual process above (API errors, permission denials, corrupted metadata), treat this as metadata unavailable.
  - Replace the JSON array with a descriptive string (e.g., `"partial_commit_download: missing 9 of 36 commits due to GitHub API failures; default to Case 2 unless other evidence suggests Case 3."`)
  - Include the missing PR numbers or SHAs in the message if known.
  - Do not pass along a truncated commit array—downstream reasoning relies on complete coverage.

Outputs
-------
- `./auto_triage/data/commit_info.json`
  - Preferred format is a JSON array of commit objects with metadata (commit hash, PR info, authors, approvers, Copilot overview, etc.).
  - **An empty array (`[]`) is a valid and expected output** when failures are non-deterministic or unrelated to tt-metal code changes. Do not force commit collection when evidence suggests the issue is external.
  - The file may also be a JSON string for the "too many commits" scenario described above.
- `./auto_triage/data/error_message.txt`
  - Contains the deterministic failure excerpt (or is empty if determinism could not be established or failures are non-deterministic).

What **Not** to Do
------------------
- Do **not** produce `explanation.md`, `slack_message.json`, or any Case outputs.
- Do **not** spend tokens summarizing commits; simply gather metadata.
- Do **not** interpret commit relevance or hypothesize about root causes—download the metadata and stop.
- Do **not** delete or rename any existing automation scripts.
- Avoid running the LLM twice; this stage is the only LLM invocation before the main analysis.

The main model will perform the final reasoning using the commit list and the captured error message.

