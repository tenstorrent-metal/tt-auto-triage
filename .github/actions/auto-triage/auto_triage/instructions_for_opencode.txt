Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
Diagnose the current failing GitHub Actions subjob on `main`. Depending on the evidence, you must choose **exactly one** of the four cases below and produce the required outputs.

Workspace Layout
----------------
- You are running inside `.auto_triage/` and all helper scripts are already staged here.
- The full `tt-metal` repository has been mirrored into `./workspace/`. Read any source files from there.
- Stay in this directory. Run helper scripts as `./get_logs.sh`, `./download_data_between_commits.sh`, etc.
- Data files live in `./auto_triage/data/` (also available via the `./data/` symlink), logs/annotations live in `./auto_triage/logs/` (`./logs/`), and outputs must go into `./auto_triage/output/` (`./output/`). Each job’s annotations (if downloaded) reside at `./logs/job_<job_id>/annotations.json`.
- Never reference parent directories (e.g., `../`). Doing so will trigger permission denials.
- Always read `./data/subjob_runs.json` (symlinked to `./auto_triage/data/subjob_runs.json`) for the list of recent runs (success vs failure). `boundaries_summary.json` is metadata only; do not rely on it for determinism checks.

-Case Matrix
------------
- **Case 1 – Failure attributable to a specific commit**
  - Only use this when you can defend a single culprit commit beyond reasonable doubt. If you still have other viable suspects, you must not choose Case 1.
- **Case 2 – Deterministic failure, culprit commit unknown**
  - The failure is deterministic but you cannot identify the exact commit (expired logs, >100 commits so the download script aborts, or other data gaps). Focus on root-cause instructions rather than naming a commit.
- **Case 3 – Failure unrelated to tt-metal changes**
  - Evidence shows the issue is probably external (infrastructure, flaky machines, mismatched dependencies, or commits that clearly cannot affect the failing test). Use this only when you have exhausted reasonable tt-metal hypotheses.
- **Case 4 – Deterministic failure with multiple plausible commits**
  - The issue clearly comes from the commit window, but the evidence cannot single out one commit with high confidence. Use this whenever you have ≥2 plausible suspects (even if one is slightly more likely).

You must justify the selected case explicitly in `./output/explanation.md`, and the report must include a subtitle like `## Case X – <case name>` so readers instantly know which scenario you chose.

Slack Message Output
--------------------
- In addition to `explanation.md`, produce exactly one `./output/slack_message.json` per run.
- The JSON **must** match the schema for the active case in `./auto_triage/slack_message_template.json`. Treat that file as the canonical contract for required top-level keys (`case`, `scenario`, `failure_message`, `failing_run_url`, `failing_run_label`, `commits`, `relevant_developers`, `notes`, `slack_message`, etc.). After writing `./output/slack_message.json`, run `./sanitize_slack_message.py ./output/slack_message.json`. The sanitizer now validates the schema and will fail if any field has the wrong type (e.g., strings instead of `{ "name": "...", "slack_id": "" }` objects). When it reports an error, fix the JSON immediately and rerun the sanitizer before proceeding.
- Populate commit objects with `hash`, `url`, `author`, `approvers`, `relevant_developers`, and `relevant_files`. When a field from the template does not apply to the chosen case (e.g., `commits` for Case 2), leave it as an empty array or omit it exactly as the template shows.
- Resolve Slack IDs via `./get_slack_ids.py` (see Toolbox). If the lookup script errors, times out, or the person cannot be found, leave that `slack_id` as an empty string `""` and document the attempt in your reasoning rather than fabricating data. Always represent people/groups as objects: `{"name":"Full Name or group-handle","slack_id":""}`. Never paste raw CLI output (like `[[],[]]`) or bare strings into `relevant_developers`, commit `author`, or `approvers`.
- Limit `relevant_files` to the most critical paths (at most five per commit or case). Prioritize files that directly illustrate the suspected failure mechanism; do not copy the entire diff file list.
- Case hints:
  - Case 1: `commits` must contain exactly one entry for the culprit commit plus any extra `relevant_developers`.
  - Case 2: `commits` stays empty, but fill the `relevant_developers`/`relevant_files` arrays with the best owners to engage.
  - Case 3: keep `commits` empty and populate `notes` to summarize why this is external.
  - Case 4: include every plausible commit (≥2) with per-commit people lists; mention confidence in the Slack summary.
- `failing_run_url` / `failing_run_label` must describe the newest run that actually exhibits the deterministic failure signature (ignore newer runs that only fail due to infra/setup issues such as runner disconnects). Use the exact job URL from `./data/subjob_runs.json` and provide a short label such as `Run #123456789 (latest reproducible failure)`.
- `slack_message` should be a concise, plain-text summary (one or two sentences) that references the case number and tells humans what to do next.
- Regardless of which case you choose (including Case 2 or Case 3), you **must** produce both `./output/explanation.md` and `./output/slack_message.json`. Do not skip these files even when the diagnosis is “insufficient data” or “external/infra issue”.
- Include both the workflow name (`${{ inputs.workflow-name }}`) and the failing test name (derived from the deterministic error you observed) prominently at the top of `explanation.md` (e.g., `Auto Triage Explanation: <workflow_name> – <test_name>`). Also populate the `workflow_name` and `failing_test_name` fields in `slack_message.json` so the Slack reporter can surface them.

Shared Workflow (Main Analysis)
-------------------------------
1. The filter stage already downloaded logs/annotations, determined the deterministic failure signature, and produced the following artifacts:
   - `./auto_triage/data/error_message.txt` – the canonical deterministic error snippet (possibly empty).
   - `./auto_triage/data/commit_info.json` – every commit between the last success and the first deterministic failure, each with `is_irrelevant`.
   - `./auto_triage/data/filtered_commit_info.json` – the subset where `is_irrelevant` remained `false`. This may be an empty array or even a JSON string (for the “too many commits” fallback). Handle each possibility explicitly in your reasoning.
2. Start by reading `filtered_commit_info.json` and `error_message.txt`. Treat that deterministic error as the anchor; do **not** re-download runs unless the existing evidence is insufficient to choose a case.
3. If you absolutely need additional logs or metadata, justify the cost clearly in `explanation.md` and stop as soon as you have enough data. Infra/setup failures identified by the filter stage must remain excluded from your determinism logic.
4. Determine whether tt-metal commits plausibly explain the failure:
   - Treat every commit in `filtered_commit_info.json` as a plausible candidate. The filter only removes clearly irrelevant entries; it does **not** imply that multiple surviving commits mean Case 4. You must still evaluate each remaining commit on its own merits and pick Case 1 whenever a single culprit can be defended.
   - Choose Case 4 only when, after reviewing the filtered commits, you still have ≥2 genuinely plausible suspects that cannot be disambiguated.
   - If `filtered_commit_info.json` is empty (or every commit was marked irrelevant) yet the failure is deterministic, lean toward Case 3.
   - If `commit_info.json` contains a JSON string instructing you to default to Case 2, follow that guidance unless other evidence clearly indicates Case 3.
5. Keep a ranked shortlist of plausible commits (including those the filter stage left as relevant). Case 1 requires exactly one remaining candidate; Case 4 requires ≥2.
6. Contact planning: while you work, note commit authors, file owners (`git log`, `git blame`, `.github/CODEOWNERS`), and capture GitHub profile URLs for everyone you may need to ping. Use the metadata already present in `commit_info.json` whenever possible.
7. In your final report, include links to the last successful job run, the first failing job run, and every surviving commit between them (use the compare URL and the filtered metadata).

Failure Evidence Rules
----------------------
- Repeated identical **test** errors across runs strongly indicate determinism but are not strictly required for Case 1. Use your judgment: if code diffs clearly explain the failure, you may proceed with a single run.
- Treat infrastructure/setup issues (runner disconnects, missing deps, timeouts before tests) as non-deterministic and exclude them entirely from the run history. Only consider runs where the tests actually executed.
- Anchor on the **most recent** run that executed the tests: its failure signature is the reference. Earlier runs only count as the start of a deterministic failure if they show the exact same test-level error.
- If there is only one test-executing failure, or no earlier run repeats the same error, treat that lone failure as the entire signal.
- If log archives lack the actual test output, assume that specific run is non-deterministic.
- Always document the failure signature and why you believe it is (or is not) attributable to tt-metal code.

Toolbox Reference
-----------------
1. **Fetch job annotations (preferred)**
   ```
   ./get_annotations.sh <job_url> [output_json]
   ```
   Saves the run’s annotations (if any) under `./logs/job_<id>/annotations.json`. Use this before touching logs.
2. **Download logs (fallback when annotations have no signal)**
   ```
   ./get_logs.sh <job_url> [output_dir]
   ```
3. **List commits between the last success and first deterministic failure**
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Creates/updates `./data/commit_info.json`. If more than 10 commits are detected it will report `BATCH_COUNT` and expect the batch script below to be called.
4. **Download commit metadata in batches (when requested)**
   ```
   ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> [output_json]
   ```
   Processes up to 10 commits per batch and appends to the specified JSON (run for indices `0..BATCH_COUNT-1`).
5. **Changed files for a commit**
   ```
   ./get_changed_files.sh <commit> [output_json]
   ```
6. **Full diff for a commit**
   ```
   ./get_commit_diff.sh <commit> [output_patch]
   ```
7. **Lookup Slack member IDs (optional)**
   ```
   ./get_slack_ids.py "Alice Smith" "bob.jones" [--limit N] [--include-bots] [--json]
   ```
   Use this to retrieve Slack IDs for commit authors, approvers, or CODEOWNERS referenced in the Slack payload. The script searches the locally cached directory at `./auto_triage/data/slack_directory.json` (generated earlier via `./download_slack_directory.py`). If the directory file is missing or stale, rerun the downloader to refresh it. If the lookup still fails (missing entry, network issues during the refresh, or unmatched names), proceed with an empty `slack_id` and document the attempt.
   Quick reference (same as the command help):
   ```
   """
   Lookup Slack member IDs for one or more developers by name.

   Usage:
     ./get_slack_ids.py "Alice Smith" "bob.jones"
   Optional flags:
     --limit N        Return up to N matches per query (default: 1).
     --include-bots   Include bot/service accounts in results.
     --json           Emit machine-readable JSON instead of a table.
   """
   ```
8. **Refresh Slack directory cache (only if needed)**
   ```
   ./download_slack_directory.py [--output auto_triage/data/slack_directory.json] [--pretty]
   ```
   Downloads all Slack users/groups your token can access and updates the directory consumed by `get_slack_ids.py`. The GitHub Action runs this automatically before the LLM step, but you can re-run it when debugging locally.
9. **Sanitize Slack payload JSON**
   ```
   ./sanitize_slack_message.py ./output/slack_message.json
   ```
   Parses the Slack payload, removes any `<content>` wrappers or other artifacts, and rewrites a clean JSON file. Always run this after generating `slack_message.json`.

Case 1 – Deterministic Failure With Identified Commit
-----------------------------------------------------
Trigger this case when:
- You have two matching test failures that prove determinism.
- The commit window between last success and first deterministic failure has ≤100 commits (batches cover anything above 5).
- A specific commit (or very small set) clearly explains the failure via Copilot overview, file diffs, etc.

Actions:
1. Run `./download_data_between_commits.sh <good> <bad>` to gather metadata.
2. Use Copilot summaries, changed files, and diffs to confirm that exactly one commit remains plausible. If any other commit still looks viable, stop and treat the whole analysis as Case 4 instead.
3. Prepare outputs under `./output/`:
   - `slack_message.json` using the Case 1 template (exactly one `commits` entry with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, populated Slack IDs when obtainable, and a concise `slack_message`).
   - `explanation.md` (title should be `Auto Triage Explanation: <failing job name>`). Cover:
     - Why the failure is deterministic.
     - The commit window, Copilot summaries referenced, and supporting diffs.
     - Exact code changes (file path + before/after hunk) from the suspected commit that plausibly trigger the observed error, with a clear explanation tying the change to the failure signature. At minimum: cite the old code (pre-commit), the new code (post-commit), and describe how the behavior change produces the observed test failure.
     - Detailed fix-forward suggestions (not just “revert”).
     - A “Who to contact” section listing names + GitHub handles + profile URLs, prioritizing the culprit commit author and relevant file owners (per `git log`, `git blame`, `.github/CODEOWNERS`).
     - Direct links to the last successful job, the first failing job, each commit in the window, and the suspected culprit commit (include the clickable commit URL in both the Slack payload and the report body).

Case 2 – Deterministic Failure But Commit Unknown
-------------------------------------------------
Use this case when a deterministic failure exists but you cannot determine the commit because:
- The oldest failing run’s logs cannot be retrieved (expired or `get_logs.sh` error) and the remaining evidence still points to a tt-metal regression rather than an external issue.
- The commit window exceeds 100 commits (script exits with “too many commits. cannot download”).
- Metadata is insufficient to map the failure to a specific change.

Actions:
1. Explain clearly why the commit cannot be identified (expired logs, excessive commit span, missing data, etc.).
2. Describe the failure signature, affected files/tests, and best-effort hypotheses on where the bug likely lives in the codebase.
3. Produce both `./output/explanation.md` and the Case 2 version of `./output/slack_message.json` (empty `commits`, populated `relevant_developers`, `relevant_files`, and a succinct `slack_message`). Include in the narrative:
   - Determinism evidence and the exact error message text (or stack trace snippet) observed in the relevant job(s).
   - Code references from the repository (file path + failing code block) that most likely cause or influence the failure, even if no specific culprit commit is known. Compare old vs. current behavior when possible.
   - Guidance for engineers on how to debug/fix the issue (files to inspect, test commands, configs to tweak).
   - A contact list (names + handles + GitHub URLs) derived from `git log`, `git blame`, `.github/CODEOWNERS`, prioritizing likely owners of the failing areas.

Case 3 – Failure Likely Outside tt-metal
----------------------------------------
Choose this case when tt-metal commits cannot reasonably explain the failure. Indicators include:
- Every failure since the last success appears non-deterministic or infrastructure-related (runner crashes, networking, flaky hardware).
- The failing runs all show identical errors, but the intervening commits only touch unrelated code paths (e.g., Python docs while the job is a C++ kernel test).
- The mismatch between failure signature and available commits leads you to conclude that the issue likely sits outside tt-metal (e.g., flaky machine, external dependency regression).
- You could not download the oldest failing run, and the remaining logs show the test never executed or failed for reasons clearly unrelated to tt-metal.

Actions:
1. Document why tt-metal is likely not at fault:
   - Summaries of the failures you inspected and the exact error messages observed.
   - Commit analysis showing no plausible offending changes.
   - Any external/environmental clues from the logs.
2. Produce `./output/explanation.md` plus the Case 3 `slack_message.json` (fills `notes` and escalation guidance). Include:
   - A clear statement that this is Case 3.
   - Suggestions for next steps (check runners, dependencies, hardware, upstream repos, etc.).
   - References to any code/config files inspected to justify that tt-metal itself is not at fault (even if only to show they remained unchanged).
   - A contact list (names + handles + GitHub URLs) for people who can help investigate the suspected area (e.g., infra owners, CODEOWNERS of the test, commit authors touching tooling).

Case 4 – Deterministic Failure With Multiple Plausible Commits
-------------------------------------------------------------------
Use this case when the failure is clearly caused by code in the commit window but you cannot defend a single culprit with high confidence. Examples:
- Two or more commits touch the failing area (same kernel/test) and their diffs each plausibly explain the error.
- Logs implicate a subsystem that was refactored by several commits in quick succession and the evidence is insufficient to isolate one.
- Copilot summaries and diffs point to overlapping risk factors and reverting only one commit would be speculative.

Actions:
1. Run the same commit download workflow as Case 1 so you understand every suspect.
2. Populate `./output/slack_message.json` using the Case 4 template: `commits` must contain every plausible commit (≥2), each with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, and Slack IDs when available. Mention the relative confidence (high/medium/low) in both the JSON narrative and the summary sentence.
3. In `./output/explanation.md`, clearly state this is Case 4, summarize the shared failure signature, and explain why each candidate commit could be responsible. Cite files + before/after hunks, link to each commit URL, and note the confidence bucket for every entry. Include guidance on how to validate or bisect the suspects sequentially.
4. Treat every candidate as if it were the Case 1 culprit: list clickable commit links, exact error tie-in, and the “who to contact” details (authors, approvers, CODEOWNERS, other file owners). After covering the per-commit info, add the overall contact section so humans know who to loop in.

Contact Guidance (All Cases)
----------------------------
- Always recommend specific engineers to loop in. Order of preference:
  1. Identified culprit commit author(s) (Case 1).
  2. Owners of files or tests implicated in the failure (from `git log`, `git blame`, `.github/CODEOWNERS`).
  3. Infra/test maintainers if Case 3 points outside tt-metal.
- If a commit author is *not* a member of the `tenstorrent` organization (`gh api orgs/tenstorrent/members/<login>` returns non-204), treat them as an external contributor: mention them for attribution, but prioritize approved reviewers inside the org for contact/escalation.
- Provide both the best-known real name (if visible) and the GitHub handle, plus a hyperlink `https://github.com/<handle>`.
- Include this section in `explanation.md` for every case.

Final Reminders
---------------
- Do not assume a failure is deterministic without evidence.
- Avoid downloading more logs than necessary; each download costs money.
- When log downloads fail, document the attempt and reason before moving on.
- Keep reasoning concise but explicit, and always tie conclusions back to logs or commit data.
- Accuracy matters—engineers will act on your findings. Choose the correct case and justify it thoroughly.
- Case 1 and Case 4 are mutually exclusive: if you can defend one culprit, emit the Case 1 Slack payload (single commit entry). If multiple suspects remain, emit the Case 4 payload (multi-commit array) and never force a single guess.
- When evidence points to multiple risky commits, explicitly say so (Case 4) instead of forcing a guess.
- If there are **no failing runs** after reading `boundaries_summary.json`, write `./output/explanation.md` stating that the triage run was invoked in error (pipeline currently passing) and exit without further analysis.
