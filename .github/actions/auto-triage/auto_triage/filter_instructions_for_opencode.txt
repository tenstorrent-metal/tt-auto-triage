Filter Stage Playbook
=====================

Purpose
-------
Identify the newest deterministic failure signature and gather the associated commit window. This stage must be exhaustive when collecting evidence, but its outputs are limited to the source metadata the next model will consume.

Available Workspace
-------------------
- You are operating inside `.auto_triage/`.
- Run helper scripts from this directory (e.g., `./get_logs.sh`, `./download_data_between_commits.sh`).
- Source files live under `./workspace/`.
- Data artifacts should be written to `./auto_triage/data/` (also symlinked at `./data/`).

Workflow Overview
-----------------
1. Inspect `./data/subjob_runs.json` (ordered newest → oldest). Identify the last successful run and the subsequent failures.
2. Download evidence exactly as follows:
   - **Oldest failing run** (first failure after the last success).
   - **Two most recent failing runs** (if fewer than three failures exist, download every failing run).
   - For each run, call `./get_annotations.sh <job_url>` first. If annotations lack actionable detail (common), immediately fall back to `./get_logs.sh <job_url>`.
3. Ignore infrastructure/setup failures (e.g., runner disconnects, missing dependencies, GitHub outages). Such runs must not influence determinism decisions.
4. Determine whether there is a repeatable deterministic failure signature:
   - Prefer an error that appears in multiple runs with the same stack trace or message.
   - If only one test-executing run exists, treat that signature as deterministic if it clearly points to tt-metal code.
   - **Important:** It is perfectly acceptable and expected for failures to be non-deterministic. If failures appear to be random, infrastructure-related, or unrelated to tt-metal code changes, this is a valid finding. Do not force determinism where none exists.
5. Save the deterministic error snippet (2–6 lines) into `./auto_triage/data/error_message.txt`. If no deterministic failure can be found, write an empty file.

Commit Metadata Collection
--------------------------
6. **Only proceed with commit collection if you identified a deterministic failure that appears related to tt-metal code changes.** If the failures are non-deterministic, infrastructure-related, or clearly unrelated to tt-metal (e.g., flaky tests, runner issues, external dependencies), skip this step entirely and leave `commit_info.json` as an empty array (`[]`).
7. Locate the last successful run and the first deterministic failing run identified above. Invoke:
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> ./auto_triage/data/commit_info.json
   ```
   - If the commit window has ≤10 commits, the script will download them directly and write to `commit_info.json`.
   - If the commit window has >10 commits, the script will output `BATCH_COUNT=<number>` (e.g., `BATCH_COUNT=5` for 50 commits).
   - When `BATCH_COUNT` is reported, you must run the batch script for each batch index from `0` to `BATCH_COUNT-1`:
     ```
     ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> ./auto_triage/data/commit_info.json
     ```
     - Example: If `BATCH_COUNT=5`, run:
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 0 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 1 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 2 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 3 ./auto_triage/data/commit_info.json`
       - `./download_data_between_commits_batch.sh <good_commit> <bad_commit> 4 ./auto_triage/data/commit_info.json`
     - **Important:** Use the same output file (`./auto_triage/data/commit_info.json`) for all batch calls; results will be appended.
     - Each batch processes up to 10 commits (batches are zero-indexed).

Special Cases
-------------
- **Purely non-deterministic / infra failures:** If every failing run is due to setup/infrastructure noise, random test flakiness, or issues clearly unrelated to tt-metal code changes, **this is a valid and acceptable outcome.** Skip downloading commits entirely. Write an empty JSON array (`[]`) to `./auto_triage/data/commit_info.json` and leave `error_message.txt` blank. This signals Case 3 to the main model, which is the correct classification for non-deterministic failures.
- **No clear deterministic failure:** If you cannot identify a repeatable failure signature across multiple runs, or if failures appear random/unrelated to code changes, treat this as non-deterministic. Leave `commit_info.json` as `[]` and `error_message.txt` blank.
- **Commit span too large (>100) or metadata unavailable:** If the helper scripts refuse to download (or logs for the oldest failing run are missing), write a JSON string into `commit_info.json`, e.g. `"too many commits to analyze; default to Case 2 unless other evidence suggests Case 3"`. Still populate `error_message.txt` with the clearest deterministic message you found (or leave it empty if none were available).

Optional Tools
--------------
- You may call helper scripts like `./get_changed_files.sh <commit>` or `./get_commit_diff.sh <commit>` if needed, but use them sparingly. The main analysis stage will perform the deep dive; this stage should only gather the commit metadata.

Outputs
-------
- `./auto_triage/data/commit_info.json`
  - Preferred format is a JSON array of commit objects with metadata (commit hash, PR info, authors, approvers, Copilot overview, etc.).
  - **An empty array (`[]`) is a valid and expected output** when failures are non-deterministic or unrelated to tt-metal code changes. Do not force commit collection when evidence suggests the issue is external.
  - The file may also be a JSON string for the "too many commits" scenario described above.
- `./auto_triage/data/error_message.txt`
  - Contains the deterministic failure excerpt (or is empty if determinism could not be established or failures are non-deterministic).

What **Not** to Do
------------------
- Do **not** produce `explanation.md`, `slack_message.json`, or any Case outputs.
- Do **not** spend tokens summarizing commits; simply gather metadata.
- Do **not** delete or rename any existing automation scripts.
- Avoid running the LLM twice; this stage is the only LLM invocation before the main analysis.

The main model will perform the final reasoning using the commit list and the captured error message.

