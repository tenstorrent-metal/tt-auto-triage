Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
Diagnose the current failing GitHub Actions subjob on `main`. Depending on the evidence, you must choose **exactly one** of the four cases below and produce the required outputs.

Workspace Layout
----------------
- You are running inside `.auto_triage/` and all helper scripts are already staged here.
- The full `tt-metal` repository has been mirrored into `./workspace/`. Read any source files from there.
- Stay in this directory. Run helper scripts as `./get_logs.sh`, `./download_data_between_commits.sh`, etc.
- Data files live in `./auto_triage/data/` (also available via the `./data/` symlink), logs/annotations live in `./auto_triage/logs/` (`./logs/`), and outputs must go into `./auto_triage/output/` (`./output/`). Each job’s annotations (if downloaded) reside at `./logs/job_<job_id>/annotations.json`.
- Never reference parent directories (e.g., `../`). Doing so will trigger permission denials.
- Always read `./data/subjob_runs.json` (symlinked to `./auto_triage/data/subjob_runs.json`) for the list of recent runs (success vs failure). `boundaries_summary.json` is metadata only; do not rely on it for determinism checks.

-Case Matrix
------------
- **Case 1 – Failure attributable to a specific commit**
  - Only use this when you can defend a single culprit commit beyond reasonable doubt. If you still have other viable suspects, you must not choose Case 1.
- **Case 2 – Deterministic failure, culprit commit unknown**
  - The failure is deterministic but you cannot identify the exact commit (expired logs, >100 commits so the download script aborts, or other data gaps). Focus on root-cause instructions rather than naming a commit.
- **Case 3 – Failure unrelated to tt-metal changes**
  - **This is a valid and acceptable outcome.** Use Case 3 when evidence shows the issue is external (infrastructure, flaky machines, mismatched dependencies, non-deterministic test failures, random test flakiness, or commits that clearly cannot affect the failing test). Case 3 is the correct classification for non-deterministic failures and should be chosen whenever failures appear random, infrastructure-related, or unrelated to tt-metal code changes. Do not force a deterministic explanation when none exists.
- **Case 4 – Deterministic failure with multiple plausible commits**
  - The issue clearly comes from the commit window, but the evidence cannot single out one commit with high confidence. Use this whenever you have ≥2 plausible suspects (even if one is slightly more likely).

You must justify the selected case explicitly in `./output/explanation.md`. The document structure must be:
- **Title:** `# Auto Triage Explanation: <job_name>` (use the job/subjob name, not the test name)
- **Subtitle:** `## Failing Test: <test_name>` (extract the test name from the deterministic error message)
- **Case section:** `## Case X – <case name>` (so readers instantly know which scenario you chose)

Slack Message Output
--------------------
- In addition to `explanation.md`, produce exactly one `./output/slack_message.json` per run.
- The JSON **must** match the schema for the active case in `./auto_triage/slack_message_template.json`. Treat that file as the canonical contract for required top-level keys (`case`, `scenario`, `failure_message`, `failing_run_url`, `failing_run_label`, `commits`, `relevant_developers`, `notes`, `slack_message`, etc.). After writing `./output/slack_message.json`, run `./sanitize_slack_message.py ./output/slack_message.json`. The sanitizer now validates the schema and will fail if any field has the wrong type (e.g., strings instead of `{ "name": "...", "slack_id": "" }` objects). When it reports an error, fix the JSON immediately and rerun the sanitizer before proceeding.
- Populate commit objects with `hash`, `url`, `author`, `approvers`, `relevant_developers`, and `relevant_files`. When a field from the template does not apply to the chosen case (e.g., `commits` for Case 2), leave it as an empty array or omit it exactly as the template shows.
- **Important distinction for `relevant_developers`:**
  - **`author` and `approvers`** (within commit objects): These are people who were directly involved in making the commit (the commit author and those who approved the PR).
  - **`relevant_developers`** (within commit objects or at top level): These should be **codeowners** of files that need to be changed to fix the issue, NOT the commit author or approvers. Use `git log`, `git blame`, or `.github/CODEOWNERS` to identify who owns the files that need modification. Only include people who would need to review/approve the fix, not those who already reviewed the problematic commit.
- **Ping behavior in Slack messages:**
  - **If commits exist (Case 1 or Case 4):** Only commit authors will be pinged (using Slack IDs). `relevant_developers` will be shown as names only (no pings).
  - **If no commits (Case 2 or Case 3):** Only top-level `relevant_developers` will be pinged (using Slack IDs).
  - **All fields must still be populated** in `slack_message.json` regardless of ping behavior - the Slack reporter handles the conditional pinging automatically.
- **Security:** **NEVER** write Slack IDs to `explanation.md`. Only include names and GitHub handles/URLs in the markdown report. Slack IDs should only exist in `slack_message.json`.
- Resolve Slack IDs as follows:
  - **For individual developers:** Use `./get_slack_ids.py` (see Toolbox) to look up Slack IDs. The script searches `./auto_triage/data/slack_directory.json` for users only.
  - **For groups/subteams:** Look directly in `./auto_triage/data/slack_groups.json`. Search for the group by name or handle, then use the `id` field as the `slack_id`. Groups have IDs starting with "S" (e.g., "S1234567890").
  - If lookup fails or the person/group cannot be found, leave `slack_id` as an empty string `""` and document the attempt in your reasoning rather than fabricating data.
  - Always represent people/groups as objects: `{"name":"Full Name or group-handle","slack_id":""}`. Never paste raw CLI output (like `[[],[]]`) or bare strings into `relevant_developers`, commit `author`, or `approvers`.
- Limit `relevant_files` to the most critical paths (at most five per commit or case). Prioritize files that directly illustrate the suspected failure mechanism; do not copy the entire diff file list.
- Case hints:
  - Case 1: `commits` must contain exactly one entry for the culprit commit. The `relevant_developers` field should list codeowners of files that need changes to fix the issue, not the commit author/approvers.
  - Case 2: `commits` stays empty, but fill the top-level `relevant_developers`/`relevant_files` arrays with the best codeowners to engage (people who own the files that need to be fixed).
  - Case 3: keep `commits` empty and populate `notes` to summarize why this is external.
  - Case 4: include every plausible commit (≥2) with per-commit people lists; mention confidence in the Slack summary.
- `failing_run_url` / `failing_run_label` must describe the newest run that actually exhibits the deterministic failure signature (ignore newer runs that only fail due to infra/setup issues such as runner disconnects). Use the exact job URL from `./data/subjob_runs.json` and provide a short label such as `Run #123456789 (latest reproducible failure)`.
- `slack_message` should be a concise, plain-text summary (one or two sentences) that references the case number and tells humans what to do next.
- Regardless of which case you choose (including Case 2 or Case 3), you **must** produce both `./output/explanation.md` and `./output/slack_message.json`. Do not skip these files even when the diagnosis is “insufficient data” or “external/infra issue”.
- The `explanation.md` title must use the **job/subjob name** (e.g., `Auto Triage Explanation: t3k llama3.2-11b-vision tests`), not the test name. The **failing test name** (extracted from the deterministic error) should appear as a subtitle (`## Failing Test: <test_name>`) before the case section. This structure makes it clear what type of job is failing while still showing the specific test. Also populate the `workflow_name` and `failing_test_name` fields in `slack_message.json` so the Slack reporter can surface them. **Note:** The `failing_job_name` field will be automatically set from the user input, so you don't need to populate it.

Shared Workflow (Main Analysis)
-------------------------------
1. The filter stage already downloaded logs/annotations, determined the deterministic failure signature, and produced the following artifacts:
   - `./auto_triage/data/error_message.txt` – the canonical deterministic error snippet (possibly empty).
   - `./auto_triage/data/commit_info.json` – every commit between the last success and the first deterministic failure. This may be an empty array or even a JSON string (for the "too many commits" fallback). Handle each possibility explicitly in your reasoning.
2. Start by reading `commit_info.json` and `error_message.txt`. Treat that deterministic error as the anchor; do **not** re-download runs unless the existing evidence is insufficient to choose a case.
3. If you absolutely need additional logs or metadata, justify the cost clearly in `explanation.md` and stop as soon as you have enough data. Infra/setup failures identified by the filter stage must remain excluded from your determinism logic.
4. Determine whether tt-metal commits plausibly explain the failure:
   - **If `commit_info.json` is empty (`[]`), this indicates the filter stage found non-deterministic or infrastructure-related failures. This is a valid finding—choose Case 3 and explain why the failures appear unrelated to tt-metal code changes.**
   - Treat every commit in `commit_info.json` as a plausible candidate. You must evaluate each commit on its own merits and pick Case 1 whenever a single culprit can be defended.
   - Choose Case 4 only when, after reviewing the commits, you still have ≥2 genuinely plausible suspects that cannot be disambiguated.
   - If `commit_info.json` contains a JSON string instructing you to default to Case 2, follow that guidance unless other evidence clearly indicates Case 3.
   - **Remember: Case 3 is a valid and acceptable outcome. Do not force a deterministic explanation when evidence points to non-deterministic, infrastructure-related, or external issues.**
5. Keep a ranked shortlist of plausible commits. Case 1 requires exactly one remaining candidate; Case 4 requires ≥2.
6. Contact planning: while you work, note commit authors, file owners (`git log`, `git blame`, `.github/CODEOWNERS`), and capture GitHub profile URLs for everyone you may need to ping. Use the metadata already present in `commit_info.json` whenever possible.
7. In your final report, include links to the last successful job run, the first failing job run, and every commit between them (use the compare URL and the commit metadata).

Failure Evidence Rules
----------------------
- **Non-deterministic failures are valid and acceptable findings.** If failures appear random, inconsistent, or infrastructure-related, choose Case 3. Do not force a deterministic explanation when none exists.
- Repeated identical **test** errors across runs strongly indicate determinism but are not strictly required for Case 1. Use your judgment: if code diffs clearly explain the failure, you may proceed with a single run.
- Treat infrastructure/setup issues (runner disconnects, missing deps, timeouts before tests) as non-deterministic and exclude them entirely from the run history. Only consider runs where the tests actually executed.
- Anchor on the **most recent** run that executed the tests: its failure signature is the reference. Earlier runs only count as the start of a deterministic failure if they show the exact same test-level error.
- If there is only one test-executing failure, or no earlier run repeats the same error, treat that lone failure as the entire signal.
- If log archives lack the actual test output, assume that specific run is non-deterministic.
- Always document the failure signature and why you believe it is (or is not) attributable to tt-metal code.

Toolbox Reference
-----------------
1. **Fetch job annotations (preferred)**
   ```
   ./get_annotations.sh <job_url> [output_json]
   ```
   Saves the run’s annotations (if any) under `./logs/job_<id>/annotations.json`. Use this before touching logs.
2. **Download logs (fallback when annotations have no signal)**
   ```
   ./get_logs.sh <job_url> [output_dir]
   ```
3. **List commits between the last success and first deterministic failure**
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Creates/updates `./data/commit_info.json`. If more than 10 commits are detected it will report `BATCH_COUNT` and expect the batch script below to be called.
4. **Download commit metadata in batches (when requested)**
   ```
   ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> [output_json]
   ```
   Processes up to 10 commits per batch and appends to the specified JSON (run for indices `0..BATCH_COUNT-1`).
5. **Changed files for a commit**
   ```
   ./get_changed_files.sh <commit> [output_json]
   ```
6. **Full diff for a commit**
   ```
   ./get_commit_diff.sh <commit> [output_patch]
   ```
7. **Lookup Slack user IDs (for individual developers only)**
   ```
   ./get_slack_ids.py "Alice Smith" "bob.jones" [--limit N] [--include-bots] [--json]
   ```
   Use this to retrieve Slack IDs for individual developers (commit authors, approvers, or CODEOWNERS). The script searches the locally cached directory at `./auto_triage/data/slack_directory.json` (generated earlier via `./download_slack_directory.py`). **This script only searches for users, not groups.**
   Quick reference (same as the command help):
   ```
   """
   Lookup Slack member IDs for one or more developers by name.

   Usage:
     ./get_slack_ids.py "Alice Smith" "bob.jones"
   Optional flags:
     --limit N        Return up to N matches per query (default: 1).
     --include-bots   Include bot/service accounts in results.
     --json           Emit machine-readable JSON instead of a table.
   """
   ```
8. **Lookup Slack group IDs (for groups/subteams)**
   - Read `./auto_triage/data/slack_groups.json` directly (this file is generated by `./download_slack_directory.py`).
   - **Important:** Group names in CODEOWNERS or other sources may not exactly match Slack group names or handles. You may need to do fuzzy matching or reasoning:
     - The CODEOWNERS entry "metalium-developers-ttnn-core" might correspond to Slack handle "ttnncore"
     - Search for partial matches: look for keywords like "ttnn", "core", "developers" in both `name` and `handle` fields
     - Check `description` fields for additional context
     - Group handles are often shorter abbreviations (e.g., "ttnncore" vs "metalium-developers-ttnn-core")
     - If unsure, try multiple search strategies: exact match first, then substring matches, then keyword matching
   - Use the `id` field as the `slack_id` in your JSON. Group IDs start with "S" (e.g., "S1234567890").
   - Example: If you need the Slack ID for "graph-runtime-owners" from CODEOWNERS, search for variations like "graph", "runtime", "owners" in the JSON and use reasoning to identify the correct group's `id` field.
9. **Refresh Slack directory cache (only if needed)**
   ```
   ./download_slack_directory.py [--output auto_triage/data/slack_directory.json] [--groups-output auto_triage/data/slack_groups.json] [--pretty]
   ```
   Downloads all Slack users and groups your token can access into separate files:
   - Users go to `./auto_triage/data/slack_directory.json` (consumed by `get_slack_ids.py`)
   - Groups go to `./auto_triage/data/slack_groups.json` (read directly for group lookups)
   The GitHub Action runs this automatically before the LLM step, but you can re-run it when debugging locally.
10. **Sanitize Slack payload JSON**
   ```
   ./sanitize_slack_message.py ./output/slack_message.json
   ```
   Parses the Slack payload, removes any `<content>` wrappers or other artifacts, and rewrites a clean JSON file. Always run this after generating `slack_message.json`.

Case 1 – Deterministic Failure With Identified Commit
-----------------------------------------------------
Trigger this case when:
- You have two matching test failures that prove determinism.
- The commit window between last success and first deterministic failure has ≤100 commits (batches cover anything above 5).
- A specific commit (or very small set) clearly explains the failure via Copilot overview, file diffs, etc.

Actions:
1. Run `./download_data_between_commits.sh <good> <bad>` to gather metadata.
2. Use Copilot summaries, changed files, and diffs to confirm that exactly one commit remains plausible. If any other commit still looks viable, stop and treat the whole analysis as Case 4 instead.
3. Prepare outputs under `./output/`:
   - `slack_message.json` using the Case 1 template (exactly one `commits` entry with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, populated Slack IDs when obtainable, and a concise `slack_message`).
   - `explanation.md` (title: `# Auto Triage Explanation: <job_name>`, subtitle: `## Failing Test: <test_name>`, then `## Case 1 – <case name>`). Cover:
     - Why the failure is deterministic.
     - The commit window, Copilot summaries referenced, and supporting diffs.
     - Exact code changes (file path + before/after hunk) from the suspected commit that plausibly trigger the observed error, with a clear explanation tying the change to the failure signature. At minimum: cite the old code (pre-commit), the new code (post-commit), and describe how the behavior change produces the observed test failure.
     - Detailed fix-forward suggestions (not just “revert”).
     - A "Who to contact" section listing names + GitHub handles + profile URLs. **Never include Slack IDs in explanation.md** - only names and GitHub information. Include the culprit commit author for attribution, but prioritize codeowners of files that need to be changed to fix the issue (from `git log`, `git blame`, `.github/CODEOWNERS`).
     - Direct links to the last successful job, the first failing job, each commit in the window, and the suspected culprit commit (include the clickable commit URL in both the Slack payload and the report body).

Case 2 – Deterministic Failure But Commit Unknown
-------------------------------------------------
Use this case when a deterministic failure exists but you cannot determine the commit because:
- The oldest failing run’s logs cannot be retrieved (expired or `get_logs.sh` error) and the remaining evidence still points to a tt-metal regression rather than an external issue.
- The commit window exceeds 100 commits (script exits with “too many commits. cannot download”).
- Metadata is insufficient to map the failure to a specific change.

Actions:
1. Explain clearly why the commit cannot be identified (expired logs, excessive commit span, missing data, etc.).
2. Describe the failure signature, affected files/tests, and best-effort hypotheses on where the bug likely lives in the codebase.
3. Produce both `./output/explanation.md` and the Case 2 version of `./output/slack_message.json` (empty `commits`, populated top-level `relevant_developers`, `relevant_files`, and a succinct `slack_message`). The `relevant_developers` field should list codeowners of files that need to be changed to fix the issue (from `git log`, `git blame`, `.github/CODEOWNERS`), not commit authors. Include in the narrative:
   - Determinism evidence and the exact error message text (or stack trace snippet) observed in the relevant job(s).
   - Code references from the repository (file path + failing code block) that most likely cause or influence the failure, even if no specific culprit commit is known. Compare old vs. current behavior when possible.
   - Guidance for engineers on how to debug/fix the issue (files to inspect, test commands, configs to tweak).
   - A contact list (names + handles + GitHub URLs) derived from `git log`, `git blame`, `.github/CODEOWNERS`, prioritizing codeowners of the files that need to be fixed. **Never include Slack IDs in explanation.md** - only names and GitHub information.

Case 3 – Failure Likely Outside tt-metal
----------------------------------------
**This is a valid and acceptable outcome. Do not hesitate to choose Case 3 when the evidence warrants it.**

Choose this case when tt-metal commits cannot reasonably explain the failure. Indicators include:
- Every failure since the last success appears non-deterministic or infrastructure-related (runner crashes, networking, flaky hardware, random test flakiness).
- Failures are inconsistent across runs with different error messages or no clear pattern.
- The failing runs all show identical errors, but the intervening commits only touch unrelated code paths (e.g., Python docs while the job is a C++ kernel test).
- The mismatch between failure signature and available commits leads you to conclude that the issue likely sits outside tt-metal (e.g., flaky machine, external dependency regression).
- You could not download the oldest failing run, and the remaining logs show the test never executed or failed for reasons clearly unrelated to tt-metal.
- **`commit_info.json` is empty (`[]`), indicating the filter stage determined failures are non-deterministic or unrelated to tt-metal code changes.**

Actions:
1. Document why tt-metal is likely not at fault:
   - Summaries of the failures you inspected and the exact error messages observed.
   - Commit analysis showing no plausible offending changes.
   - Any external/environmental clues from the logs.
2. Produce `./output/explanation.md` plus the Case 3 `slack_message.json` (fills `notes` and escalation guidance). Include:
   - A clear statement that this is Case 3.
   - Suggestions for next steps (check runners, dependencies, hardware, upstream repos, etc.).
   - References to any code/config files inspected to justify that tt-metal itself is not at fault (even if only to show they remained unchanged).
   - A contact list (names + handles + GitHub URLs) for people who can help investigate the suspected area (e.g., infra owners, CODEOWNERS of the test). **Never include Slack IDs in explanation.md** - only names and GitHub information. If you include commit authors, it should be for context only, not as the primary contacts.

Case 4 – Deterministic Failure With Multiple Plausible Commits
-------------------------------------------------------------------
Use this case when the failure is clearly caused by code in the commit window but you cannot defend a single culprit with high confidence. Examples:
- Two or more commits touch the failing area (same kernel/test) and their diffs each plausibly explain the error.
- Logs implicate a subsystem that was refactored by several commits in quick succession and the evidence is insufficient to isolate one.
- Copilot summaries and diffs point to overlapping risk factors and reverting only one commit would be speculative.

Actions:
1. Run the same commit download workflow as Case 1 so you understand every suspect.
2. Populate `./output/slack_message.json` using the Case 4 template: `commits` must contain every plausible commit (≥2), each with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, and Slack IDs when available. For each commit, `relevant_developers` should list codeowners of files that need changes to fix the issue, NOT the commit author/approvers. Mention the relative confidence (high/medium/low) in both the JSON narrative and the summary sentence.
3. In `./output/explanation.md`, clearly state this is Case 4, summarize the shared failure signature, and explain why each candidate commit could be responsible. Cite files + before/after hunks, link to each commit URL, and note the confidence bucket for every entry. Include guidance on how to validate or bisect the suspects sequentially.
4. Treat every candidate as if it were the Case 1 culprit: list clickable commit links, exact error tie-in, and the "who to contact" details. Include commit authors/approvers for attribution, but prioritize codeowners of files that need to be changed (from `git log`, `git blame`, `.github/CODEOWNERS`). **Never include Slack IDs in explanation.md** - only names and GitHub information. After covering the per-commit info, add the overall contact section so humans know who to loop in.

Contact Guidance (All Cases)
----------------------------
- Always recommend specific engineers to loop in. Order of preference:
  1. **Codeowners of files that need to be changed to fix the issue** (from `git log`, `git blame`, `.github/CODEOWNERS`). These are the people who should review/approve the fix.
  2. Identified culprit commit author(s) (Case 1) - include for attribution/context, but they may not be the right people to fix it.
  3. Infra/test maintainers if Case 3 points outside tt-metal.
- **Important:** The `relevant_developers` field in `slack_message.json` should list codeowners who need to review the fix, NOT the commit author or approvers (those go in `author` and `approvers` fields).
- If a commit author is *not* a member of the `tenstorrent` organization (`gh api orgs/tenstorrent/members/<login>` returns non-204), treat them as an external contributor: mention them for attribution, but prioritize approved reviewers inside the org for contact/escalation.
- Provide both the best-known real name (if visible) and the GitHub handle, plus a hyperlink `https://github.com/<handle>`.
- **Never include Slack IDs in explanation.md** - this is a security concern. Slack IDs should only exist in `slack_message.json`.
- Include this section in `explanation.md` for every case.

Final Reminders
---------------
- Do not assume a failure is deterministic without evidence.
- Avoid downloading more logs than necessary; each download costs money.
- When log downloads fail, document the attempt and reason before moving on.
- Keep reasoning concise but explicit, and always tie conclusions back to logs or commit data.
- Accuracy matters—engineers will act on your findings. Choose the correct case and justify it thoroughly.
- Case 1 and Case 4 are mutually exclusive: if you can defend one culprit, emit the Case 1 Slack payload (single commit entry). If multiple suspects remain, emit the Case 4 payload (multi-commit array) and never force a single guess.
- When evidence points to multiple risky commits, explicitly say so (Case 4) instead of forcing a guess.
- If there are **no failing runs** after reading `boundaries_summary.json`, write `./output/explanation.md` stating that the triage run was invoked in error (pipeline currently passing) and exit without further analysis.
