Auto Triage Playbook (LLM-Friendly)
==================================

Mission
-------
Diagnose the current failing GitHub Actions subjob on `main`. Depending on the evidence, you must choose **exactly one** of the four cases below and produce the required outputs.

**IMPORTANT: Job Name Reference**
- The prompt above says "for workflow 'X' and job 'Y'" - use these exact values:
  - **Job name for title:** Use 'Y' (the job name from the prompt) exactly as provided. This is what goes in `explanation.md` title.
  - **Workflow name:** Use 'X' (the workflow name from the prompt) for the `workflow_name` field in `slack_message.json`.
  - **DO NOT populate `failing_job_name`** in `slack_message.json` - it is automatically set from the user input.

Workspace Layout
----------------
- You are running inside `.auto_triage/` and all helper scripts are already staged here.
- The full `tt-metal` repository has been mirrored into `./workspace/`. Read any source files from there.
- Stay in this directory. Run helper scripts as `./get_logs.sh`, `./download_data_between_commits.sh`, etc.
- Data files live in `./auto_triage/data/` (also available via the `./data/` symlink), logs/annotations live in `./auto_triage/logs/` (`./logs/`), and outputs must go into `./auto_triage/output/` (`./output/`). Each job’s annotations (if downloaded) reside at `./logs/job_<job_id>/annotations.json`.
- Never reference parent directories (e.g., `../`). Doing so will trigger permission denials.
- Always read `./data/subjob_runs.json` (symlinked to `./auto_triage/data/subjob_runs.json`) for the run history. Newer runs write it as `{ "status": "...", "message": "...", "runs": [ ... ] }`; legacy runs might still be a bare JSON array of run objects. If you encounter the legacy format, treat the whole array as `runs` and assume `status: "ok"`. The `status`/`message` (when present) tells you whether the boundary finder hit a fallback (`ok`, `failure_limit_exceeded`, `no_success_found`). Use this file for run metadata; detailed commit reasoning still comes from `commit_info.json` and the other artifacts produced by the filter stage.
-Case Matrix
------------
- **Case 1 – Failure attributable to a specific commit**
  - Only use this when you can defend a single culprit commit beyond reasonable doubt **after functionally vetting every other commit in the window**. You must either (a) inspect the actual code changes for each commit and write explicit reasoning for why it cannot cause the failure, or (b) apply clearly articulated, conservative rules for quickly discarding commits (e.g., “docs-only change while failure is in a C++ kernel”). Simple `grep`/`rg` searches, filename-only inspection, or shallow summaries are **not** enough to justify Case 1. If you still have other viable or insufficiently-analyzed suspects, you must not choose Case 1 (use Case 4 instead).
- **Case 2 – Deterministic failure, culprit commit unknown**
  - The failure is deterministic but you cannot identify the exact commit (expired logs, >100 commits so the download script aborts, or other data gaps). Focus on root-cause instructions rather than naming a commit.
- **Case 3 – Failure unrelated to tt-metal changes**
  - **This is a valid and acceptable outcome.** Use Case 3 when evidence shows the issue is external (infrastructure, flaky machines, mismatched dependencies, non-deterministic test failures, random test flakiness, or commits that clearly cannot affect the failing test). Case 3 is the correct classification for non-deterministic failures and should be chosen whenever failures appear random, infrastructure-related, or unrelated to tt-metal code changes. Do not force a deterministic explanation when none exists.
- **Case 4 – Deterministic failure with multiple plausible commits**
  - The issue clearly comes from the commit window, but the evidence cannot single out one commit with high confidence. Use this whenever you have ≥2 plausible suspects (even if one is slightly more likely).
- **Case 5 – Deterministic failure with incomplete commit metadata (best-effort culprit)**
  - The failure is deterministic and clearly tied to the commit window, but some commits in the range could not be downloaded even after retries/manual backfill. You must still rank all **available** commits by likelihood, pick the best candidate, and explicitly acknowledge that unseen commits might actually be responsible.

You must justify the selected case explicitly in `./output/explanation.md`. The document structure must be:
- **Title:** `# Auto Triage Explanation: <job_name>` where `<job_name>` is **the exact job name provided in the prompt above** (e.g., if the prompt says "for workflow 'X' and job 'Y'", use 'Y' exactly as provided). **DO NOT use the test name or workflow name in the title - use the job name from the prompt.**
- **Subtitle:** `## Failing Test: <test_name>` (extract the test name from the deterministic error message)
- **Case section:** `## Case X – <case name>` (so readers instantly know which scenario you chose)

Slack Message Output
--------------------
- In addition to `explanation.md`, produce exactly one `./output/slack_message.json` per run.
- The JSON **must** match the schema for the active case in `./auto_triage/slack_message_template.json`. Treat that file as the canonical contract for required top-level keys (`case`, `scenario`, `failure_message`, `failing_run_url`, `failing_run_label`, `commits`, `relevant_developers`, `notes`, `slack_message`, etc.). After writing `./output/slack_message.json`, run `./sanitize_slack_message.py ./output/slack_message.json`. The sanitizer now validates the schema and will fail if any field has the wrong type (e.g., strings instead of `{ "name": "...", "slack_id": "" }` objects) or if required fields are missing. When it reports an error, fix the JSON immediately and rerun the sanitizer before proceeding.
- **For any case that uses commits (1, 4, or 5), you MUST assign and output a numeric confidence score between 0 and 100 for every single commit in the window.** This means:
  - Every commit object you include in `commits` **must** have a `confidence` field (0–100).
  - The `commits` array **must** contain an entry for every commit in `commit_info.json` (when `commit_info.json` is a JSON array), not just plausible ones. Commits that are clearly irrelevant should be given very low scores (often 0), but they must still appear with an explicit numeric score.
  - The `commits` array must be ordered in **descending** confidence (highest score at the top).
- Populate commit objects with `hash`, `url`, `author`, `approvers`, `relevant_developers`, and `relevant_files`. When a field from the template does not apply to the chosen case (e.g., `commits` for Case 2 or Case 3), leave it as an empty array or omit it exactly as the template shows.
- **CRITICAL: `relevant_developers` rules - READ CAREFULLY:**
  - **`author` and `approvers`** (within commit objects): These are people who were directly involved in making the commit (the commit author and those who approved the PR).
  - **`relevant_developers`** (within commit objects or at top level): 
    - **MUST be codeowners** of files that need to be changed to fix the issue.
    - **MUST NOT include** the commit author or any approvers from the same commit.
    - **MUST NOT include** any person who is already listed as `author` or `approvers` in any commit in the same `slack_message.json`.
    - **CAN be empty** if you cannot find appropriate codeowners or recent file modifiers. It's better to leave it empty than to incorrectly list authors/approvers.
    - Use `git log`, `git blame`, or `.github/CODEOWNERS` to identify who owns the files that need modification.
    - Only include people who would need to review/approve the fix, not those who already reviewed the problematic commit.
    - **No duplicates:** Each person should appear at most once across all `relevant_developers` lists in the entire JSON.
- **Ping behavior in Slack messages:**
  - **If commits exist (Case 1 or Case 4):** Only commit authors will be pinged (using Slack IDs). `relevant_developers` will be shown as names only (no pings).
  - **If no commits (Case 2 or Case 3):** Only top-level `relevant_developers` will be pinged (using Slack IDs).
  - **If commits exist but commit metadata is incomplete (Case 5):** Commit authors **and** top-level `relevant_developers` will be pinged. This explicitly calls out both the most likely culprit(s) and the codeowners who should double-check the partial analysis.
  - **All fields must still be populated** in `slack_message.json` regardless of ping behavior - the Slack reporter handles the conditional pinging automatically.
- **Security:** **NEVER** write Slack IDs to `explanation.md`. Only include names and GitHub handles/URLs in the markdown report. Slack IDs should only exist in `slack_message.json`.
- Resolve Slack IDs as follows:
  - **For individual developers:** Use `./get_slack_ids.py` (see Toolbox) to look up Slack IDs. The script searches `./auto_triage/data/slack_directory.json` for users only.
  - **For groups/subteams:** Look directly in `./auto_triage/data/slack_groups.json`. Search for the group by name or handle, then use the `id` field as the `slack_id`. Groups have IDs starting with "S" (e.g., "S1234567890").
  - If lookup fails or the person/group cannot be found, leave `slack_id` as an empty string `""` and document the attempt in your reasoning rather than fabricating data.
  - Always represent people/groups as objects: `{"name":"Full Name or group-handle","slack_id":""}`. Never paste raw CLI output (like `[[],[]]`) or bare strings into `relevant_developers`, commit `author`, or `approvers`.
- Limit `relevant_files` to the most critical paths (at most five per commit or case). Prioritize files that directly illustrate the suspected failure mechanism; do not copy the entire diff file list.
- Case hints:
  - Case 1: `commits` must contain exactly one entry for the culprit commit. The `relevant_developers` field **MUST NOT** include the commit author or any approvers. It should only list codeowners of files that need changes to fix the issue. If you cannot find appropriate codeowners, leave `relevant_developers` as an empty array `[]`.
  - Case 2: `commits` stays empty, but fill the top-level `relevant_developers`/`relevant_files` arrays with the best codeowners to engage (people who own the files that need to be fixed). If you cannot find appropriate codeowners, leave `relevant_developers` as an empty array `[]`.
  - Case 3: keep `commits` empty and populate `notes` to summarize why this is external. **Do not list any `relevant_developers`** (leave the array empty). The Slack workflow automatically pings the Metal Infra on-call for Case 3 when the list is empty; adding developers here would ping them unnecessarily.
  - Case 4: include every plausible commit (≥2) with per-commit people lists; mention confidence in the Slack summary.
- `failing_run_url` / `failing_run_label` must describe the newest run that actually exhibits the deterministic failure signature (ignore newer runs that only fail due to infra/setup issues such as runner disconnects). Use the exact job URL from `./data/subjob_runs.json` and provide a short label such as `Run #123456789 (latest reproducible failure)`.
- `slack_message` should be a concise, plain-text summary (one or two sentences) that tells humans what to do next. Do NOT include the Case number in this text.
- Regardless of which case you choose (including Case 2 or Case 3), you **must** produce both `./output/explanation.md` and `./output/slack_message.json`. Do not skip these files even when the diagnosis is “insufficient data” or “external/infra issue”.
- The `explanation.md` title must use **the exact job name provided in the prompt** (the prompt says "for workflow 'X' and job 'Y'" - use 'Y' exactly as shown). **DO NOT use the test name, workflow name, or try to infer the job name from other sources.** The **failing test name** (extracted from the deterministic error) should appear as a subtitle (`## Failing Test: <test_name>`) before the case section. Example: If the prompt says "job 't3k llama3.2-11b-vision tests'", the title should be `# Auto Triage Explanation: t3k llama3.2-11b-vision tests`. Also populate the `workflow_name` (from the prompt) and `failing_test_name` fields in `slack_message.json` so the Slack reporter can surface them. **CRITICAL:** Do **NOT** populate `failing_job_name` in `slack_message.json` - it is automatically set from the user input and will be wrong if you try to set it yourself.

Shared Workflow (Main Analysis)
-------------------------------
1. The filter stage already downloaded logs/annotations, determined the deterministic failure signature, and produced the following artifacts:
   - `./auto_triage/data/error_message.txt` – the canonical deterministic error snippet (possibly empty).
   - `./auto_triage/data/commit_info.json` – every commit between the last success and the first deterministic failure. This may be an empty array or even a JSON string (for the "too many commits" fallback). Handle each possibility explicitly in your reasoning.
2. Start by reading error_message.txt. Treat that deterministic error as the anchor. Using that error, and the knowledge of what test failed, determine what files and systems you believe are relevant in the failure. Specifically look through source files in tt-metal to determine likely sources of broken code. You must use these files to guide your analysis as you read through information on past commits.
3. Then read commit_info.json; do **not** re-download runs unless the existing evidence is insufficient to choose a case.
4. If you absolutely need additional logs or metadata, justify the cost clearly in `explanation.md` and stop as soon as you have enough data. Infra/setup failures identified by the filter stage must remain excluded from your determinism logic.
5. Determine whether tt-metal commits plausibly explain the failure:
   - **If `commit_info.json` is empty (`[]`), this indicates the filter stage found non-deterministic or infrastructure-related failures. This is a valid finding—choose Case 3 and explain why the failures appear unrelated to tt-metal code changes.**
   - **Check Known Patterns:** You should also double-check `non-deterministic-error-examples.json` if it exists. Use `grep` or `rg` to search for the error message in that file. If a match is found, this strongly confirms Case 3.
     The file format is:
     ```json
     { "examples": [ {"job": "...", "test": "...", "error_message": "..." } ] }
     ```
   - Remember, the job and test that a past non deterministic issue appeared on is less important then the error itself. Often very different jobs and tests will fail as a result of the same non-deterministic issue. You may have to use fuzzy matching or reasoning to determine if two errors are the same because every test's output is slightly different
  - Treat every commit in `commit_info.json` as a plausible candidate. You must evaluate each commit on its own merits by looking at the **actual code diffs and changed files that could influence the failing test**, not just by running a few searches. Case 1 is only allowed when every other commit has been explicitly ruled out with clear, code-based reasoning or safely discarded via conservative rules (e.g., comments/docs-only changes).
  - Choose Case 4 whenever, after this review, ≥2 genuinely plausible suspects remain that you cannot confidently disambiguate. When in doubt between Case 1 and Case 4, prefer Case 4.
  - If `commit_info.json` contains a JSON string (e.g., `too many commits...` or `failure_limit_exceeded: ...`), obey the instruction. Strings indicate the filter stage could not gather a structured commit list, so **only Case 2 or Case 3 are allowed**—decide between them based on whether the error appears deterministic/tt-metal-related (Case 2) or external/non-deterministic (Case 3). This same limitation is announced in `subjob_runs.json.status` when applicable; make sure your final case selection respects it.
  - If `./auto_triage/data/commit_download_status.json` exists and reports a `"status"` such as `"partial_commit_download"` or any non-zero `missing_commits`/`expected_commits - downloaded_commits`, treat commit metadata as **incomplete**. In that situation you must not claim full coverage (Cases 1 or 4); instead, you may choose between Case 2, Case 3, or the partial-information **Case 5** depending on whether the failure is tt-metal-related and whether the available commits still support a plausible culprit.
  - **Remember: Case 3 is a valid and acceptable outcome. Do not force a deterministic explanation when evidence points to non-deterministic, infrastructure-related, or external issues.**
6. For any case that uses commits (1, 4, or 5), you **must** assign a numeric **confidence score** between 0 and 100 to **every commit in `commit_info.json`** (when it is a JSON array), even if that score is 0. It is acceptable to use conservative rules to bulk-assign very low scores (e.g., 0 or 5) to obviously irrelevant commits (docs-only changes, clearly unrelated subsystems), but **no commit is allowed to remain unscored**. Keep a ranked shortlist ordered from highest to lowest confidence and:
   - Before deciding on Case 1, your ranked list **must** contain exactly one remaining candidate with a very high score (above 95) and you **must** have written down why **every other commit** in the window both (a) has a lower confidence (strictly below 90) and (b) cannot reasonably cause the failure.
   - If any commit is still under-analyzed or only ruled out via shallow checks, you are not allowed to choose Case 1; treat the situation as Case 4 or Case 5 instead (Case 4 requires ≥2 well-motivated candidates with similar confidence, Case 5 is reserved for incomplete commit metadata).
7. Contact planning: while you work, note commit authors, file owners (`git log`, `git blame`, `.github/CODEOWNERS`), and capture GitHub profile URLs for everyone you may need to ping. Use the metadata already present in `commit_info.json` whenever possible, and ensure that any ranked commit list you present in `explanation.md` includes both the numeric confidence scores and the rationale behind the ordering. In your final report, include a **markdown table** that lists **every commit in the window** (hash, brief description, and confidence score from 0 to 100), sorted from highest to lowest confidence, so humans can quickly see the full distribution of suspects.
8. In your final report, include links to the last successful job run, the first failing job run, and every commit between them (use the compare URL and the commit metadata).

Failure Evidence Rules
----------------------
- **Non-deterministic failures are valid and acceptable findings.** If failures appear random, inconsistent, or infrastructure-related, choose Case 3. Do not force a deterministic explanation when none exists.
- Repeated identical **test** errors across runs strongly indicate determinism but are not strictly required for Case 1. Use your judgment: if code diffs clearly explain the failure, you may proceed with a single run.
- Treat infrastructure/setup issues (runner disconnects, missing deps, timeouts before tests) as non-deterministic and exclude them entirely from the run history. Only consider runs where the tests actually executed.
- Anchor on the **most recent** run that executed the tests: its failure signature is the reference. Earlier runs only count as the start of a deterministic failure if they show the exact same test-level error.
- If there is only one test-executing failure, or no earlier run repeats the same error, treat that lone failure as the entire signal.
- If log archives lack the actual test output, assume that specific run is non-deterministic.
- Always document the failure signature and why you believe it is (or is not) attributable to tt-metal code.

Toolbox Reference
-----------------
1. **Fetch job annotations (preferred)**
   ```
   ./get_annotations.sh <job_url> [output_json]
   ```
   Saves the run’s annotations (if any) under `./logs/job_<id>/annotations.json`. Use this before touching logs.
2. **Download logs (fallback when annotations have no signal)**
   ```
   ./get_logs.sh <job_url> [output_dir]
   ```
3. **List commits between the last success and first deterministic failure**
   ```
   ./download_data_between_commits.sh <good_commit> <bad_commit> [output_json]
   ```
   Creates/updates `./data/commit_info.json`. If more than 10 commits are detected it will report `BATCH_COUNT` and expect the batch script below to be called.
4. **Download commit metadata in batches (when requested)**
   ```
   ./download_data_between_commits_batch.sh <good_commit> <bad_commit> <batch_index> [output_json]
   ```
   Processes up to 10 commits per batch and appends to the specified JSON (run for indices `0..BATCH_COUNT-1`).
5. **Changed files for a commit**
   ```
   ./get_changed_files.sh <commit> [output_json]
   ```
6. **Full diff for a commit**
   ```
   ./get_commit_diff.sh <commit> [output_patch]
   ```
7. **Lookup Slack user IDs (for individual developers only)**
   ```
   ./get_slack_ids.py "Alice Smith" "bob.jones" [--limit N] [--include-bots] [--json]
   ```
   Use this to retrieve Slack IDs for individual developers (commit authors, approvers, or CODEOWNERS). The script searches the locally cached directory at `./auto_triage/data/slack_directory.json` (generated earlier via `./download_slack_directory.py`). **This script only searches for users, not groups.**
   Quick reference (same as the command help):
   ```
   """
   Lookup Slack member IDs for one or more developers by name.

   Usage:
     ./get_slack_ids.py "Alice Smith" "bob.jones"
   Optional flags:
     --limit N        Return up to N matches per query (default: 1).
     --include-bots   Include bot/service accounts in results.
     --json           Emit machine-readable JSON instead of a table.
   """
   ```
8. **Lookup Slack group IDs (for groups/subteams)**
   - Read `./auto_triage/data/slack_groups.json` directly (this file is generated by `./download_slack_directory.py`).
   - **Important:** Group names in CODEOWNERS or other sources may not exactly match Slack group names or handles. You may need to do fuzzy matching or reasoning:
     - The CODEOWNERS entry "metalium-developers-ttnn-core" might correspond to Slack handle "ttnncore"
     - Search for partial matches: look for keywords like "ttnn", "core", "developers" in both `name` and `handle` fields
     - Check `description` fields for additional context
     - Group handles are often shorter abbreviations (e.g., "ttnncore" vs "metalium-developers-ttnn-core")
     - If unsure, try multiple search strategies: exact match first, then substring matches, then keyword matching
   - Use the `id` field as the `slack_id` in your JSON. Group IDs start with "S" (e.g., "S1234567890").
   - Example: If you need the Slack ID for "graph-runtime-owners" from CODEOWNERS, search for variations like "graph", "runtime", "owners" in the JSON and use reasoning to identify the correct group's `id` field.
9. **Refresh Slack directory cache (only if needed)**
   ```
   ./download_slack_directory.py [--output auto_triage/data/slack_directory.json] [--groups-output auto_triage/data/slack_groups.json] [--pretty]
   ```
   Downloads all Slack users and groups your token can access into separate files:
   - Users go to `./auto_triage/data/slack_directory.json` (consumed by `get_slack_ids.py`)
   - Groups go to `./auto_triage/data/slack_groups.json` (read directly for group lookups)
   The GitHub Action runs this automatically before the LLM step, but you can re-run it when debugging locally.
10. **Sanitize Slack payload JSON**
   ```
   ./sanitize_slack_message.py ./output/slack_message.json
   ```
   Parses the Slack payload, removes any `<content>` wrappers or other artifacts, and rewrites a clean JSON file. Always run this after generating `slack_message.json`.

11. **List commits between two SHAs (metadata-only)**
    ```
    ./list_commits_between.sh <good_commit> <bad_commit> [output_json]
    ```
    Produces a JSON array of objects `{ "sha": "...", "short": "...", "subject": "..." }` either to stdout or to the specified file. Use this when you need an authoritative list of SHAs in the window (for coverage checks or manual backfill) without hitting GitHub APIs.

12. **Download metadata for a single commit**
    ```
    ./download_data_for_single_commit.sh <commit_sha> [output_json]
    ```
    Uses the same schema as the batch downloader to fetch metadata for **one** commit at a time and append it to the specified JSON (default: `./data/commit_info.json`). Prefer this when a single problematic commit keeps causing batch downloads to fail so you can isolate it without corrupting the rest of the batch.

Case 1 – Deterministic Failure With Identified Commit
-----------------------------------------------------
Trigger this case when:
- You have two matching test failures that prove determinism.
- The commit window between last success and first deterministic failure has ≤100 commits (batches cover anything above 5).
- A specific commit (or very small set) clearly explains the failure via Copilot overview, file diffs, etc.
- **All other commits in the window have been functionally evaluated and ruled out with explicit reasoning.** For each non-culprit commit, you should be able to point to the relevant code changes and explain why they cannot realistically produce the observed failure signature.

Actions:
1. Run `./download_data_between_commits.sh <good> <bad>` to gather metadata.
2. For **every commit** in the window, use Copilot summaries, changed files, and especially **full diffs** of the relevant areas to decide whether that commit could plausibly cause the observed failure. Do not discard a commit until you have looked at the actual code changes that could affect the failing test and written down why they do or do not line up with the failure signature. Simple `grep`/`rg` usage or filename-only inspection is **never** sufficient to eliminate a commit for Case 1.
3. Remember not to get tunnel visioned. It's very easy to just choose one commit that may have caused the failure and then try to fit the failure to that commit, but this is likely to be inaccurate. If you're working towards Case 1, you **must** go through EVERY COMMIT in the window and either (a) functionally rule it out with explicit reasoning tied to its code changes, or (b) keep it on the plausible list. One of your greatest weaknesses in the past is that you wouldn't even look at the commit that ended up being the source of the error before deeply analyzing and choosing a different commit that wasn't actually relevant.
4. When ≥2 commits still look plausible after individual review, you **must directly compare them** instead of treating them in isolation. For each remaining candidate, explain how its specific code changes would produce the observed failure signature, then compare those explanations side-by-side and decide whether one mechanism is clearly more consistent with the logs/tests than the others. If, after this direct comparison, more than one plausible commit remains, you **must** stop and treat the analysis as Case 4 instead of forcing a single guess.
5. Prepare outputs under `./output/`:
   - `slack_message.json` using the Case 1 template (exactly one `commits` entry with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, populated Slack IDs when obtainable, and a concise `slack_message`). **CRITICAL:** The `relevant_developers` field in the commit object **MUST NOT** include the commit author or any approvers. Only include codeowners who need to review the fix. If you cannot find appropriate codeowners, use an empty array `[]`.
   - `explanation.md` (title: `# Auto Triage Explanation: <job_name>` where `<job_name>` is **the exact job name from the prompt** - do NOT use test name or workflow name, subtitle: `## Failing Test: <test_name>`, then `## Case 1 – <case name>`). Cover:
     - Why the failure is deterministic.
     - The commit window, Copilot summaries referenced, and supporting diffs. You must list **EVERY COMMIT** that was in the window and include at least 1 sentence explaining why each commit besides the one you have chosen is not the source of the error
     - Exact code changes (file path + before/after hunk) from the suspected commit that plausibly trigger the observed error, with a clear explanation tying the change to the failure signature. At minimum: cite the old code (pre-commit), the new code (post-commit), and describe how the behavior change produces the observed test failure.
     - Detailed fix-forward suggestions (not just “revert”).
     - A "Who to contact" section listing names + GitHub handles + profile URLs. **Never include Slack IDs in explanation.md** - only names and GitHub information. Include the culprit commit author for attribution, but prioritize codeowners of files that need to be changed to fix the issue (from `git log`, `git blame`, `.github/CODEOWNERS`).
     - Direct links to the last successful job, the first failing job, each commit in the window, and the suspected culprit commit (include the clickable commit URL in both the Slack payload and the report body).

Case 2 – Deterministic Failure But Commit Unknown
-------------------------------------------------
Use this case when a deterministic failure exists but you cannot determine the commit because:
- The oldest failing run’s logs cannot be retrieved (expired or `get_logs.sh` error) and the remaining evidence still points to a tt-metal regression rather than an external issue.
- The commit window exceeds 100 commits (script exits with “too many commits. cannot download”).
- Metadata is insufficient to map the failure to a specific change.

Actions:
1. Explain clearly why the commit cannot be identified (expired logs, excessive commit span, missing data, etc.).
2. Describe the failure signature, affected files/tests, and best-effort hypotheses on where the bug likely lives in the codebase.
3. Produce both `./output/explanation.md` and the Case 2 version of `./output/slack_message.json` (empty `commits`, populated top-level `relevant_developers`, `relevant_files`, and a succinct `slack_message`). The `relevant_developers` field should list codeowners of files that need to be changed to fix the issue (from `git log`, `git blame`, `.github/CODEOWNERS`), not commit authors. Include in the narrative:
   - Determinism evidence and the exact error message text (or stack trace snippet) observed in the relevant job(s).
   - Code references from the repository (file path + failing code block) that most likely cause or influence the failure, even if no specific culprit commit is known. Compare old vs. current behavior when possible.
   - Guidance for engineers on how to debug/fix the issue (files to inspect, test commands, configs to tweak).
   - A contact list (names + handles + GitHub URLs) derived from `git log`, `git blame`, `.github/CODEOWNERS`, prioritizing codeowners of the files that need to be fixed. **Never include Slack IDs in explanation.md** - only names and GitHub information.

Case 3 – Failure Likely Outside tt-metal
----------------------------------------
**This is a valid and acceptable outcome. Do not hesitate to choose Case 3 when the evidence warrants it.**

Choose this case when tt-metal commits cannot reasonably explain the failure. Indicators include:
- Every failure since the last success appears non-deterministic or infrastructure-related (runner crashes, networking, flaky hardware, random test flakiness).
- Failures are inconsistent across runs with different error messages or no clear pattern.
- The failing runs all show identical errors, but the intervening commits only touch unrelated code paths (e.g., Python docs while the job is a C++ kernel test).
- The mismatch between failure signature and available commits leads you to conclude that the issue likely sits outside tt-metal (e.g., flaky machine, external dependency regression).
- You could not download the oldest failing run, and the remaining logs show the test never executed or failed for reasons clearly unrelated to tt-metal.
- **`commit_info.json` is empty (`[]`), indicating the filter stage determined failures are non-deterministic or unrelated to tt-metal code changes.**

Actions:
1. Document why tt-metal is likely not at fault:
   - Summaries of the failures you inspected and the exact error messages observed.
   - Commit analysis showing no plausible offending changes.
   - Any external/environmental clues from the logs.
2. Produce `./output/explanation.md` plus the Case 3 `slack_message.json` (fills `notes` and escalation guidance). Include:
   - A clear statement that this is Case 3.
   - Suggestions for next steps (check runners, dependencies, hardware, upstream repos, etc.).
   - References to any code/config files inspected to justify that tt-metal itself is not at fault (even if only to show they remained unchanged).
   - A contact list (names + handles + GitHub URLs) for people who can help investigate the suspected area (e.g., infra owners, CODEOWNERS of the test). **Never include Slack IDs in explanation.md** - only names and GitHub information. If you include commit authors, it should be for context only, not as the primary contacts.

Case 4 – Deterministic Failure With Multiple Plausible Commits
-------------------------------------------------------------------
Use this case when the failure is clearly caused by code in the commit window but you cannot defend a single culprit with high confidence. Examples:
- Two or more commits touch the failing area (same kernel/test) and their diffs each plausibly explain the error.
- Logs implicate a subsystem that was refactored by several commits in quick succession and the evidence is insufficient to isolate one.
- Copilot summaries and diffs point to overlapping risk factors and reverting only one commit would be speculative.

Actions:
1. Run the same commit download workflow as Case 1 so you understand every suspect.
2. Populate `./output/slack_message.json` using the Case 4 template: `commits` must contain every plausible commit (≥2), each with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, and Slack IDs when available. **CRITICAL:** For each commit, `relevant_developers` **MUST NOT** include that commit's author or any of its approvers. Only include codeowners of files that need changes to fix the issue. If you cannot find appropriate codeowners for a commit, use an empty array `[]` for that commit's `relevant_developers`. Mention the relative confidence (high/medium/low) in both the JSON narrative and the summary sentence.
3. In `./output/explanation.md`, clearly state this is Case 4, summarize the shared failure signature, and explain why each candidate commit could be responsible. Cite files + before/after hunks, link to each commit URL, and note the confidence bucket for every entry. Include guidance on how to validate or bisect the suspects sequentially.
4. Treat every candidate as if it were the Case 1 culprit: list clickable commit links, exact error tie-in, and the "who to contact" details. Include commit authors/approvers for attribution, but prioritize codeowners of files that need to be changed (from `git log`, `git blame`, `.github/CODEOWNERS`). **Never include Slack IDs in explanation.md** - only names and GitHub information. After covering the per-commit info, add the overall contact section so humans know who to loop in.

Case 5 – Deterministic Failure With Incomplete Commit Metadata (Best-Effort Culprit)
------------------------------------------------------------------------------------
Use this case when the failure is clearly deterministic and tied to the commit window, but **commit metadata is incomplete** even after retries and manual backfill (for example, `commit_download_status.json` reports `"partial_commit_download"` or non-zero missing commits). You still have a non-empty set of commits in `commit_info.json` to reason about, but you cannot guarantee that every commit in the true window is represented.

Actions:
1. Treat the available commits exactly as you would for Case 1/Case 4: examine actual diffs, changed files, and Copilot summaries to decide which commits plausibly explain the failure. Assign each commit a numeric confidence score between 0 and 100 and keep the list sorted from highest to lowest confidence.
2. If one commit clearly stands out among the **available** commits, you may present it as the best-effort culprit, but you **must** explicitly call out in both `explanation.md` and `slack_message.json` that some commits could not be downloaded and that the true culprit might be among the missing ones.
3. Populate `./output/slack_message.json` using the Case 5 template: include every plausible available commit (≥1) in the `commits` array with `hash`, `url`, `author`, `approvers`, `relevant_developers`, `relevant_files`, and a numeric `confidence` field. Ensure the array is sorted in descending confidence. Top-level `relevant_developers` should list codeowners who should double-check the analysis; for Case 5 both commit authors and these codeowners will be pinged.
4. In `./output/explanation.md`, clearly state this is Case 5, summarize the deterministic failure signature, describe which commits you considered and their confidence scores, and explain why you believe the highlighted commit(s) are the most likely cause **given the partial metadata**. Include counts from `commit_download_status.json` (expected vs. downloaded commits, list of missing SHAs/PRs when known) so humans understand the residual risk.
5. Provide explicit guidance for follow-up validation (e.g., targeted bisects across the most suspicious available commits, re-running the job after regenerating metadata, or manually inspecting the missing SHAs if/when they become available).

Contact Guidance (All Cases)
----------------------------
- Always recommend specific engineers to loop in. Order of preference:
  1. **Codeowners of files that need to be changed to fix the issue** (from `git log`, `git blame`, `.github/CODEOWNERS`). These are the people who should review/approve the fix.
  2. Identified culprit commit author(s) (Case 1) - include for attribution/context, but they may not be the right people to fix it.
  3. Infra/test maintainers if Case 3 points outside tt-metal.
- **Important:** The `relevant_developers` field in `slack_message.json` should list codeowners who need to review the fix, NOT the commit author or approvers (those go in `author` and `approvers` fields).
- If a commit author is *not* a member of the `tenstorrent` organization (`gh api orgs/tenstorrent/members/<login>` returns non-204), treat them as an external contributor: mention them for attribution, but prioritize approved reviewers inside the org for contact/escalation.
- Provide both the best-known real name (if visible) and the GitHub handle, plus a hyperlink `https://github.com/<handle>`.
- **Never include Slack IDs in explanation.md** - this is a security concern. Slack IDs should only exist in `slack_message.json`.
- Include this section in `explanation.md` for every case.

Auto Fix Framework
------------------
There is an optional follow-up stage that can ask Copilot to draft a PR automatically. Use it **only when you are absolutely certain it will succeed.**

1. File location: `./auto_triage/create_PR_boolean.json` (inside your workspace). The file contains a single key `create_PR` and defaults to `false`. If the file is missing, create it with `{ "create_PR": false }`.
2. Eligibility:
   - Applies **only** to **Case 1** or **Case 2** determinations.
   - You must be **extremely confident** that the fix is understood, can be implemented in **under 100 lines of code**, and touches **no more than 3 files**.
   - Assume the downstream Copilot instance will only have access to your `explanation.md`. If that document does not clearly define actionable steps, do **not** enable auto fix.
3. How to opt in:
   - When (and only when) all eligibility criteria are satisfied, set `create_PR` to `true` in `create_PR_boolean.json`.
   - Otherwise, leave it as `false`. When in doubt, choose `false`.
4. Justification:
   - Regardless of the value you choose, include a short note in `explanation.md` explaining **why** you enabled or disabled auto-fix for this run (e.g., “Auto-fix enabled: single-line guard missing in `foo.cpp`” or “Auto-fix disabled: requires multi-file refactor”).
5. Consequences:
   - When `create_PR` is `true`, the action will trigger a follow-up Copilot delegate run that attempts to create a **draft PR** in `tenstorrent/tt-metal`. Incorrectly enabling this wastes money and time, so err on the side of leaving it `false`.

Final Reminders
---------------
- Do not assume a failure is deterministic without evidence.
- Avoid downloading more logs than necessary; each download costs money.
- When log downloads fail, document the attempt and reason before moving on.
- Keep reasoning concise but explicit, and always tie conclusions back to logs or commit data.
- Accuracy matters—engineers will act on your findings. Choose the correct case and justify it thoroughly.
- Case 1 and Case 4 are mutually exclusive: if you can defend one culprit, emit the Case 1 Slack payload (single commit entry). If multiple suspects remain, emit the Case 4 payload (multi-commit array) and never force a single guess.
- When evidence points to multiple risky commits, explicitly say so (Case 4) instead of forcing a guess.
- If there are **no failing runs** after reading `boundaries_summary.json`, write `./output/explanation.md` stating that the triage run was invoked in error (pipeline currently passing) and exit without further analysis.
